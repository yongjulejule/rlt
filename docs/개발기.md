# rlt Dev-Log

# 20230918

- Git 을 frontend - backend 로 분할
	- Frontend: CLI  로 subcommand, 파싱 후 필요한 작업을 적절한 포멧으로 만들어 backend request
		- user friendly 한 UI 제공
		- `햣 ㅁㅇㅇ .` 해도 작동하는 frontend? `git commut` 이런거 오타 수정 해주기?? 
		- DTO 는 backend 를 해봐야 감이 올듯 
		- subcommand 별 help 잘 알려주기
		- 설계
			- 파싱?  / 분류 / request? (가장 많이 바뀌는 부분 -------- Backend 와 소통하는 부분)
	- Backend: 받은 request 를 적절히 처리 후 file 에 write. (파일 기반이지만 DB 기반으로 바꿔치기 해도 작동하게 할 수 있지 않을까? 대박 완전 클린아키텍쳐)
		- 멀라... request 받는 곳 - 실제 데이터 저장 - 응답
		- verbose 모드 지원
		- git add 를 하면 어떤 일이 일어나는지 쉽게 알 수 있도록
		- 최소 단위의 operation 으로 쪼갤 수 있지 않을까? 머... 파일 하나 읽기 -> 인코딩하기 -> object 생성 -> .git/path/to/save 에 저장 이런식으로 ... 

- CLI - 일단 CLI 지만, UI 라고 생각할 수 있을듯. GUI APP 이 될 수 있고, WEB 이 될 수 있고 등등
	- sdk or lib 처럼 만들어서, Add, Commit 같은걸 객체로 사용할 수 있게. 
		- 흠... chacha 생각해보기 


---
- 설계
	- 흠... 몰라 git hash-object 부터 냅다 갈긴다

---
- sha 1 & sha 256 지원...? 일단 sha1 로 gogo 

너무 막막하니 TDD 해야겠지?? 테스트는 영원한 네비게이터

---
TDD 는 안했는데 뭔가 완성됐다.

blob 를 만들어보는 방식으로 git add 를 하면 벌어지는 일을 구현함


# 20230919
러스트 하... 먼가 조금씩 아쉽다.
cfg = configuration 
dyn = dynamic
등등 익숙하지 않은 줄임말이 넘 많다. WNOHANG <- 이게 나은듯;  그래도 awk 나 AVL 같은 것보단 직관적이다.  저거보다 안직관적일수 없긴 해

그리고 Unit test 를 같은 파일에 작성한다. 원래 세상이 그런가? 내가... 테스트를 JS 랑 C++ 밖에 안해본 사람이라 너무 불편한건가? Java 는 어떰? 걔네도 데코레이터로 하던데 -> 응 얘네도 다른파일에 해 

하... 공식문서에서 TDD 를 안내할정도면 나름 진심인거 같은데 쩝... 아 근데 nest test 작성할때 module resolve 하던거 생각하니까 갑자기 맘에 들어졌다. 굿굿인듯 ㅋㅋ 

ㄹ러스트 공식문서에 cli 툴 만들기가 있네? 도랏 ㅎ https://rust-cli.github.io/book/tutorial/cli-args.html

홀수날은 프론트 짝수날은 백엔드 하면 될듯 ㅋㅋ

rust-clap 쓰는데 어렵다 진심 러스트 아직도 1도 모르겠다

---

# 20230920
근데 날짜 왜쓰지? 걍 커밋이 날짜아님? blame 갈기면 될텐데

충격대박꿀팁) unix socket 은 git add 해도 반응이 없다. named pipe 도 안받는다ㅏ. 

# 20230921

내가 진짜 오늘 init 구현한다.

걍 .rlt 파일 만들고 그안에 몇개 넣으면 되는거 아님? 
git init 두번하면 어케함?
.git 파일중 몇개만 지우면 어캄?

# 20230922

Blob -> hash-object 로 변환중...

# 20230923

그렇다. `.git` 은 DB 이며 Backend 의 Storage 다. 나머지 파일은 다 client 다. 그러했던것이다. 댑악.

어... 흠... 클라가 git hash-object -w test-file 을 입력하면 서버는 파일시스템에서 test-file 을 찾아 읽고, hash 후 .git/object 에 저장을 한다. 이게 흠... 같은 FS 긴 한데 먼가 구분되지 않나? .git/ 같이 core 한 부분이 저장되는 곳 && Client 가 자신의 파일을 들고와서 요청을 날리는 곳... 일케..

파일의 이름만 들고 왔을때 그걸 앞단에서 파일을 읽은 뒤 core 로 넘겨주는게 나은가? 아님 core 부분에서 read / write 를 다해버려? 

일단 filename 만 받아서 처리한다 치면 아래와 같음. 근데... 만약 workspace 가 외부에 있다면? 어캄? client 가 파일 내용을 담아서 요청을 보낸다면? 

![[client-with-workspace-and-core]]
[Excalidraw](../Excalidraw/client-with-workspace-and-core.excalidraw)

여기서 workspace 가 plugin architecture 처럼... 되는건가? 아님 말고 ㅋㅋ


<details> <summary> 
ChatGPT 한테 물어봤따. GPT 는 신이다.
</summary>

Designing a remote-enabled `git hash-object` functionality can be a fun challenge. Let's break this down.

### Design Considerations:

1. **Separation of Concerns**: Keep the logic of hashing separate from the logic of obtaining the file content. Whether the content comes from a local filesystem or over the network should be abstracted away.

2. **Data Sources**: You'll essentially have three data sources:
    - Local filesystem
    - Direct file content passed in the request
    - Remote client's filesystem via an HTTP endpoint

3. **Consistency**: Regardless of the source of the file content, the hashing mechanism and return values should be consistent.

### Proposed Design:

1. **File Content Retrieval Interface**: Design an interface (in Rust, a trait) that abstracts away the source of the file content. This trait might look something like:

    ```rust
    trait FileContentProvider {
        fn get_content(&self, filename: &str) -> Result<Vec<u8>, SomeErrorType>;
    }
    ```

2. **Implementations of the Interface**:
    - `LocalFilesystemProvider`: Reads content from the local filesystem.
    - `DirectContentProvider`: Uses the file content passed directly in the request.
    - `RemoteClientProvider`: Retrieves content from a remote client's filesystem via HTTP.

3. **Hashing Logic**: Implement the hashing logic as a standalone function or a module that takes in file content (in the form of bytes) and returns the hash. This function/module should not care where the content comes from.

4. **Endpoint Logic**: If you're providing this functionality via an HTTP API:
    - Allow clients to either send the filename (in which case, you use the `RemoteClientProvider`) or send the file content directly (using `DirectContentProvider`).
    - Based on what the client sends, pick the right provider to fetch the content, then hash it, and return the hash.

5. **Command Line Logic**: For command-line use:
    - If given just a filename, use the `LocalFilesystemProvider`.
    - If there's a way to accept raw content (maybe from stdin or as a direct argument), use the `DirectContentProvider`.

### Sample Flow for HTTP API:

1. **Client Sends Filename**:
    - Client sends a request like `POST /hash-object` with a JSON body `{ "filename": "path/to/file/on/client" }`.
    - Your server uses the `RemoteClientProvider` to fetch the file content.
    - The content is then passed to the hashing function/module.
    - The hash is returned to the client.

2. **Client Sends Direct Content**:
    - Client sends a request like `POST /hash-object` with a JSON body `{ "content": "raw file content here..." }`.
    - Your server uses the `DirectContentProvider` to access the provided content.
    - The content is then passed to the hashing function/module.
    - The hash is returned to the client.

The main idea is to keep each component focused on one task. The hashing logic doesn't care about where the content comes from. The content providers don't care about what you do with the content once it's retrieved. This separation of concerns makes the design cleaner, more modular, and easier to extend or modify in the future.

 </details>

![[hash-object-from-gpt.excalidraw]]
[Excalidraw](../Excalidraw/hash-object-from-gpt.excalidraw)

- 각 Port 로 요청을 받으면 필요한 provider 를 생성하고 필요 시 잠시 저장해둔다. 
- Core 에선 WorkspaceProvider 로 추상화 된 인터페이스를 통해 접근한다. 
- 대박~

# 20230924

git cat-file 구현중...

object hash 도 받을 수 있고 branch 명도 받을 수 있는데 어캄?

이게 40자리 16진수인지 해서 isObject() 이런거?  흠... storage port 에서 해야하나? 근데 이거 core 아닌가? 흠... sha1 인지 sha256 인지도 판단해야하고 branch name 이나 HEAD 같은거면 refs 에서 읽어야 할 것 같은데? 

```
dest_filename = null
if not object 
	object_hash = read from refs
	dest_filename = "objects/" + object_hash[:2] + object_hash[2:]
else
	dest_filename = "objects/" + object_hash[:2] + object_hash[2:]
```

엄... 근데 HEAD~1 이딴건 어캄? 망 `git cat-file blob main:README.md` 이런것도 되는데?

main 을 commit 으로 읽기 -> 여기서 README.md 는 어케찾음? ㅋㅋ README.md 를 hash 해서 값 비교한다음.. 몰라? 

소스코드 보는데 쩝... 쉽지않다

- https://github.com/git/git/blob/master/git.c#L695
- https://github.com/git/git/blob/master/object-file.c

# 20230925

일단 cat-file 구현하자. 근데 그전에... store 를 wrapping 할 필요가 있을 것 같다.

SomeObjectWichInteractWithGitDir 이런 거. helper 쯤 되나?

.git 의 온갖 정보를 갖고 있으면서, 어느정도 setup 해둔 친구

필요한 setup
-> config 보고 필요한거 설정

object hash 로 read/write 하면 알아서 [..2]/[2..] 로 interact? 
흠... getObjectFileName  이런거 있어야 하나? 

액션
- (normal file|memory|DB|network) I/O
계산
- object filename format 변경, ... 엄... main^3 이런거 파싱, 아! git revision 봐야지
데이터
- ㅁㄹ



아 머리 개아프네? 

Core 라는 친구가 있다.  얘는 .git 안에 있는 내용을 알고 있음 

처음에 execute 되기 전에 config 를 읽어서 setup 을 하는데.... 근데 모든 command 가 config 를 알 필요는 없음. 

config 에 올 수 있는게 뭐가 있는지 다 알아야 할까? 

근데, 필요할때 lazy loading 시키는것도 가능하지 않을까? Core 라는 객체 안에서, 만약 read-object 요청이 들어오면, object-format 을 읽고... 그거에 맞게 object 를 read 해야하는데... 흠; 좋은건지 몰겟다 

그럼 그냥 무조건 setup 을 한다고 치고, 흠... interface 로 어떻게 해주는게 좋을까? 
진짜 input/output 만 날리는게 낫지 않나?

index / pack 부분을 더 공부해야하나?

일단은... read(object-hash | revision) 일케 받고 함수 안에서 머...
```
fn get_hash_from_revision() {
	if is_revision() {
		resolve_revision(target)
	} else {
		target
	}
}
fn read(target: String) {
	let object_hash = get_hash_from_revision()
	self.store.read(hash)
}
```

이러면 근데 쩝... store 를 굳이 다른 객체로 분리해놔야하나? 
DI 를 의도한거긴 해서 테스트가 편하긴 할텐데, core 생성할때  store 를 넣어주는 방식이 되고 어디서도 더이상 재활용 될 것 같지 않은데?? Client storage 는 Provider 로 해결해야지... 같은 storage 를 써도 논리적으로 나눠놓는게 맞는듯; 

아님... core 랑 store 를 나눠둬야 하나? 이걸 나누면? 장점이? 음... 
Storage I/O 와 core login 분리? 근데 이거 분리하면 command execute 할때 `let object = core.resolve_revision(); store.read(object)` 일케 될텐데 오히려 I/O 부분이 더 노출되는게 아닌지? 아니지 오히려 위로 빼는게 맞나?? 


갓-GPT 랑 2시간동안 싸워봤다. 

<details> <summary> git add . flow 와 layered architecture </summary> 

Let's break down the steps you've described for the `git add .` command:

1. **Command Parsing**: This is about interpreting the user's input. It's typically considered an outer layer since it interacts directly with the user interface (in this case, the CLI).
    
2. **Configure Git (read .git dir)**: Reading configurations can be seen as a type of data access, which is typically an outer concern. However, understanding and applying those configurations are core business logic concerns.
    
3. **Read Files in Current Dir**: This is a data access operation. It's about retrieving the current state of the workspace, so it's an outer layer concern.
    
4. **Update Index & Objects in .git dir**: This involves both business logic (understanding what changes have been made, which files to track, how to hash them, etc.) and data access (writing to the `.git` directory). The business logic aspects are inner concerns, while the actual IO operations are outer concerns.
    

### Layering:

From innermost to outermost, I'd generally structure it like this:

1. **Innermost (Core Business Logic)**:
    
    - Understanding and applying git configurations
    - Understanding changes in the workspace (e.g., which files were added/removed/modified)
    - Deciding how to update the index and object store based on detected changes
2. **Middle (Application Logic)**:
    
    - Coordinating the steps needed to perform a `git add` operation
    - Orchestration of the core business operations with the necessary data access
3. **Outermost (Data Access and IO)**:
    
    - Parsing the command from the CLI
    - Reading from and writing to the filesystem
    - Reading the `.git` configuration

### Recommendations:

1. **Core Business Logic**: This layer should have no direct dependencies on external frameworks or systems. It contains pure logic that understands git operations. It would include classes or modules to represent git concepts (commits, trees, blobs) and operations on them.
    
2. **Application Logic**: Acts as a coordinator. It uses the core business logic to execute commands, obtains necessary data from the outer layers, and ensures the results are stored or displayed appropriately.
    
3. **Data Access and IO**: These are your adapters or ports. They communicate with the outside world, be it reading files, interacting with the user, or even potential future features like networking operations (if you were to implement something like `git fetch` from a remote).

</details> 

## 결론

## Data-Access Layer
- DataStore (FileStore, RemoteHttpStore, MemoryStore)
- 각 Manager 에 대응하는 부분으로 쪼갤지는 고민해보기
## Application Layer (Domain Logic)
- RefManager - interact with `.git/refs, HEAD`
- ObjectManager interact with `.git/objects`
- ConfigManager interact with remain configs

## Core (Core Business Logic)
- Core object (hash, compress, rev-parse ...)

<details><summary>"그 친구" 의 리뷰</summary>

결론이 잘 정리된 것처럼 보입니다. 분석을 검토해 보겠습니다:

### 데이터 액세스 레이어:

서로 다른 구현(`FileStore`, `RemoteHttpStore`, `MemoryStore`)을 가진 **DataStore**를 사용하는 것이 합리적입니다. 이렇게 하면 데이터를 검색하거나 저장하는 방법에 대한 세부 사항이 추상화됩니다. 이 계층에서 각 관리자의 해당 부분을 분할하는 것을 고려하는 것에 대한 메모도 좋은 지적이지만, 명확성을 더할지 아니면 불필요한 복잡성을 가져올지 평가해야 합니다.

### 애플리케이션 계층(도메인 로직):

정의한 관리자는 git 아키텍처를 고려할 때 논리적으로 의미가 있습니다:

- RefManager**: 참조를 처리하는 것은 Git의 핵심 부분이므로 이 로직을 분리하면 명확성과 유지 관리에 도움이 될 수 있습니다.
  
- 오브젝트 관리자**: 오브젝트 데이터베이스에는 커밋, 트리, 블롭, 태그가 저장되므로 이 부분은 Git의 핵심입니다. 이 로직을 분리하면 유지 관리에도 도움이 됩니다.
  
- 컨피그 매니저**: Git에서는 구성이 방대하고 다양할 수 있으므로 고유한 도메인 로직 클래스에 유지하는 것이 좋습니다.

### 코어(핵심 비즈니스 로직):

해싱, 압축, 리비전 구문 분석과 같은 기본적인 Git 작업을 위한 메서드가 있는 **Core Object**는 코어 계층의 목적에 잘 부합합니다. 

몇 가지 추가 생각:

1. **인터페이스 일관성**: 다른 관리자와 스토어에서 노출되는 메서드가 해당되는 경우 일관된 인터페이스를 가지고 있는지 확인하세요. 이렇게 하면 컴포넌트 교체와 테스트가 훨씬 쉬워집니다.

2. **디커플링**: 의존성 반전 원칙을 명심하세요. 코어와 애플리케이션 레이어는 구체적인 구현이 아닌 추상화(인터페이스/특성)에 의존해야 합니다. 이렇게 하면 예를 들어 `FileStore`를 `RemoteHttpStore`로 쉽게 바꿀 수 있습니다.

3. **에러 처리**: Git은 참조 누락부터 IO 오류까지 다양한 에러를 발생시킬 수 있습니다. 사용자 정의 오류 유형을 사용하여 레이어 전체에서 오류를 처리하는 통합된 방법을 고려하세요.

4. **확장성**: Git의 시스템을 자세히 살펴보고 더 고급 기능을 구현하고 싶다고 언급했듯이, 새로운 기능을 비교적 쉽게 추가할 수 있는 방식으로 아키텍처를 구성하세요.

5. **테스트**: 이러한 계층적 접근 방식을 사용하면 테스트가 더 간단해집니다. '데이터스토어'를 모의하여 애플리케이션 계층을 테스트하고, 관리자를 모의하여 코어 계층을 테스트할 수 있습니다. 

전반적으로 구조화된 접근 방식은 모듈화 및 유지보수를 유지하면서 Git과 유사한 시스템을 구현하기 위한 명확한 로드맵을 제공합니다. 다른 아키텍처와 마찬가지로 구현을 더 깊이 파고들면 조정이 필요한 영역을 발견할 수 있지만, 이것은 시작하기에 탄탄한 기초입니다!

</details>


# 20230930

Clean Architecture 쪽으로 생각해야겠다. Hexagonal 은 잘 모르기도 하는데 대부분 DDD 특화라... 애매한듯

몰라! 일단 구현하자 고민해도 답안나온다

# 20231007

열심히 갈아엎었다

infrastructure - 바깥 세계에 직접 접근
adapter - wrapper for infra & bridge to use-case
use-cases - 비즈니스 로직
entities - ....애매함 

adapter 에 object manager 를 만들어서 구현했는데 막상 만들어 두니 이상하다.
blob object 만 일단 생각하고 validation 까지 다 adapter 에 넣어버렸는데 이러면 adapter 가 너무 많은 책임을 지는 것 같고 use-case 에 더 가까워 보인다. 근데 또 이걸 cat-file 에 넣자니.... git-object 와 상호작용 하는게 cat-file 만 있는게 아닐 것이다. 

결국 object-manager 와 유사한 use-case 가 또 필요한가? 생각이 드는데... 어캄? 이게 맞나?

일단 util 함수로 뺴볼까? 


# 20231008

여러개의 file I/O 가 필요한데 이거 어케하는게 효휼적일까? Hybrid 로 하는 방식도 있다.

## 방법 1

Async 로 다 때려박고 promise all (비슷한거 ㅎ) 갈기기

## 방법 2

Multithread 로 다 때려박고 pthread_join (비슷한거 ㅎ) 갈기기

## 방법 3

1. Async I/O 로 File 을 읽는다
2. File content 를 다 memory 에 올려 두기 (대충 Array? HashMap?)
3. 각 content 에 대하여 multithreading 으로 hash / compression 하기
4. ... 근데 다시 file 에 써야하는데? 또 다시 Async I/O.... 를 한다?

## 결론

다해보고 퍼포먼스 체크해보자 ㅎ 아키텍쳐를 잘 짰으면 바꾸기 쉽겠지? 잘... 짜야겠지? 

그리고 local file 에 접근하는 경우에, 고성능 SSD 를 사용한다면 async 가 딱히 효과 없을 수 있다고 함.ㅇㄴㅁ 